import sys
import os
import shutil
import torch
import pickle
from types import ModuleType

# ==============================================================================
# 1. [í•„ìˆ˜] ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ëª¨ë¸ Mocking (ì—ëŸ¬ ë°©ì§€)
# ==============================================================================
def mock_lib_with_class(module_name, class_names=[]):
    if module_name in sys.modules: return sys.modules[module_name]
    m = ModuleType(module_name)
    sys.modules[module_name] = m
    for cls_name in class_names:
        if not hasattr(m, cls_name): setattr(m, cls_name, type(cls_name, (object,), {}))
    return m

# ì˜ì¡´ì„± ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì§œ ë“±ë¡
for lib in ['kmeans_pytorch', 'lightgbm', 'xgboost', 'ray', 'hyperopt', 'colorama']:
    mock_lib_with_class(lib)
    sys.modules[f"{lib}.sklearn"] = mock_lib_with_class(f"{lib}.sklearn")

if 'kmeans_pytorch' in sys.modules:
    sys.modules['kmeans_pytorch'].kmeans = lambda *args, **kwargs: (None, None)

# RecBole ë‚´ë¶€ ëª¨ë¸ ë° TiSASRec ê²½ë¡œ ì—°ê²°
mock_lib_with_class('recbole.model.general_recommender.ldiffrec', ['LDiffRec'])
mock_lib_with_class('recbole.model.general_recommender.diffrec', ['DiffRec'])

import tisasrec_local
sys.modules['TiSASRec'] = tisasrec_local

# RecBole Utils íŒ¨ì¹˜
import recbole
if not hasattr(recbole, 'utils'):
    recbole.utils = ModuleType('recbole.utils')
    sys.modules['recbole.utils'] = recbole.utils
if not hasattr(recbole.utils, 'enum_type'):
    m_enum = ModuleType('recbole.utils.enum_type')
    sys.modules['recbole.utils.enum_type'] = m_enum
    recbole.utils.enum_type = m_enum
    class Dummy:
        def __init__(self, *args, **kwargs): pass
    for cls in ['ModelType', 'DataLoaderType', 'KGDataLoaderState', 'EvaluatorType', 'InputType', 'FeatureType']:
        setattr(m_enum, cls, Dummy)

# ==============================================================================
# 2. ë°ì´í„° ê²½ë¡œ í™•ì¸ ë° ë³´ì •
# ==============================================================================
def prepare_data_folder():
    dataset_name = 'amazon-data' 
    filename = f'{dataset_name}.inter'
    
    current_path = os.path.join('data', filename)
    target_dir = os.path.join('data', dataset_name)
    target_path = os.path.join(target_dir, filename)
    
    if os.path.exists(target_path): return dataset_name
    if os.path.exists(current_path):
        os.makedirs(target_dir, exist_ok=True)
        shutil.move(current_path, target_path)
        return dataset_name
        
    print(f"âŒ '{filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)

# ==============================================================================
# 3. ìœ ì € ë§¤í•‘ ì¶”ì¶œ ë¡œì§
# ==============================================================================
from recbole.config import Config
from recbole.data import create_dataset

def extract_user_vocab():
    dataset_name = prepare_data_folder()
    
    print("ğŸš€ ìœ ì € ë§¤í•‘(User ID Mapping) ë³µì› ì‹œì‘...")
    
    # 1. ëª¨ë¸ ì„¤ì • ë¡œë“œ
    pth_path = 'data/TiSASRec-Nov-28-2025_09-45-58.pth'
    checkpoint = torch.load(pth_path, map_location='cpu', weights_only=False)
    saved_config = checkpoint['config']
    
    # 2. í•„í„°ë§ ì¡°ê±´ ì„¤ì • (í•™ìŠµ ë•Œì™€ ë™ì¼í•˜ê²Œ 5íšŒ)
    # ì•„ì´í…œ ê°œìˆ˜ë¥¼ ë§ì·„ë˜ ê·¸ ì¡°ê±´ê³¼ ë™ì¼í•´ì•¼ ìœ ì € IDë„ ë§ìŠµë‹ˆë‹¤.
    min_user = 5
    min_item = 5
    max_len = saved_config['MAX_ITEM_LIST_LENGTH'] if 'MAX_ITEM_LIST_LENGTH' in saved_config else 50

    config_dict = {
        'data_path': 'data/',           
        'dataset': dataset_name,  
        'gpu_id': -1,                   
        'show_progress': False,
        'min_user_inter': min_user,
        'min_item_inter': min_item,
        'MAX_ITEM_LIST_LENGTH': max_len,
        'train_neg_sample_args': None, 
        'neg_sampling': None,
        'load_col': {'inter': ['user_id', 'item_id', 'timestamp']}
    }
    
    print(f"â„¹ï¸ í•„í„°ë§ ì¡°ê±´: User >= {min_user}, Item >= {min_item}")
    
    # 3. ë°ì´í„°ì…‹ ìƒì„±
    config = Config(model='SASRec', config_dict=config_dict)
    dataset = create_dataset(config)
    
    # 4. ìœ ì € ë§¤í•‘ ì¶”ì¶œ (ì—¬ê¸°ê°€ í•µì‹¬!)
    # dataset.field2token_id['user_id'] : ì›ë³¸ ìœ ì €ID -> ëª¨ë¸ ìœ ì €ID
    # dataset.field2id_token['user_id'] : ëª¨ë¸ ìœ ì €ID -> ì›ë³¸ ìœ ì €ID
    user_token2id = dataset.field2token_id['user_id']
    user_id2token = dataset.field2id_token['user_id']
    
    print("-" * 50)
    print(f"âœ… ì¶”ì¶œ ì™„ë£Œ!")
    print(f" - ì´ ìœ ì € ìˆ˜ (ëª¨ë¸ í•™ìŠµ ê¸°ì¤€): {len(user_token2id)}")
    
    # 5. ì €ì¥
    output_path = "data/user_vocab.pkl"
    with open(output_path, "wb") as f:
        pickle.dump({
            'user_token2id': user_token2id,
            'user_id2token': user_id2token
        }, f)
        
    print(f"âœ… ìœ ì € ë§¤í•‘ íŒŒì¼ ì €ì¥ë¨: {output_path}")

if __name__ == "__main__":
    extract_user_vocab()